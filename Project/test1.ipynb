{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import statistics as st \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove listing_id  \n",
    "#### listing_id is all unique without duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['listing_id', 'property_details_url', 'elevation','property_name','address','title'])\n",
    "test = test.drop(columns=['listing_id', 'property_details_url', 'elevation','property_name','address','title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distance to commericial centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdistance = pd.read_csv('auxiliary-data/sg-commerical-centres.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized haversine function\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['cc_distance'] = haversine_np(train['lng'], train['lat'], ccdistance['lng'], ccdistance['lat'])\n",
    "expected_result = pd.merge(train, ccdistance, on = 'planning_area')\n",
    "expected_result.head()\n",
    "train['cc_distance'] = haversine_np(train['lng'], train['lat'], expected_result['lng_y'], expected_result['lat_y'])\n",
    "\n",
    "# train['cc_distance'] = haversine_np(train['lng'], train['lat'], ccdistance['lng'], ccdistance['lat'])\n",
    "expected_result = pd.merge(test, ccdistance, on = 'planning_area')\n",
    "expected_result.head()\n",
    "test['cc_distance'] = haversine_np(test['lng'], test['lat'], expected_result['lng_y'], expected_result['lat_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdistancemean = train['cc_distance'].mean()\n",
    "train['cc_distance']=train['cc_distance'].fillna(ccdistancemean)\n",
    "\n",
    "ccdistancemean = test['cc_distance'].mean()\n",
    "test['cc_distance']=test['cc_distance'].fillna(ccdistancemean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distance to mrt stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using long and lat\n",
    "mrtdistance = pd.read_csv('auxiliary-data/sg-mrt-stations.csv')\n",
    "expected_result = pd.merge(train, mrtdistance, on = 'planning_area')\n",
    "train['mrt_distance'] = haversine_np(train['lng'], train['lat'], expected_result['lng_y'], expected_result['lat_y'])\n",
    "\n",
    "\n",
    "# using long and lat\n",
    "mrtdistance = pd.read_csv('auxiliary-data/sg-mrt-stations.csv')\n",
    "expected_result = pd.merge(test, mrtdistance, on = 'planning_area')\n",
    "test['mrt_distance'] = haversine_np(test['lng'], test['lat'], expected_result['lng_y'], expected_result['lat_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distance to primary school "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using long and lat\n",
    "primaryschooldistance = pd.read_csv('auxiliary-data/sg-primary-schools.csv')\n",
    "expected_result = pd.merge(train, primaryschooldistance, on = 'planning_area')\n",
    "train['primary_school_distance'] = haversine_np(train['lng'], train['lat'], expected_result['lng_y'], expected_result['lat_y'])\n",
    "\n",
    "# using long and lat\n",
    "primaryschooldistance = pd.read_csv('auxiliary-data/sg-primary-schools.csv')\n",
    "expected_result = pd.merge(test, primaryschooldistance, on = 'planning_area')\n",
    "test['primary_school_distance'] = haversine_np(test['lng'], test['lat'], expected_result['lng_y'], expected_result['lat_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distance to secondary school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using long and lat\n",
    "secschooldistance = pd.read_csv('auxiliary-data/sg-secondary-schools.csv')\n",
    "expected_result = pd.merge(train, secschooldistance, on = 'planning_area')\n",
    "train['secondary_school_distance'] = haversine_np(train['lng'], train['lat'], expected_result['lng_y'], expected_result['lat_y'])\n",
    "\n",
    "# using long and lat\n",
    "secschooldistance = pd.read_csv('auxiliary-data/sg-secondary-schools.csv')\n",
    "expected_result = pd.merge(test, secschooldistance, on = 'planning_area')\n",
    "test['secondary_school_distance'] = haversine_np(test['lng'], test['lat'], expected_result['lng_y'], expected_result['lat_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distance to shopping malls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using long and lat\n",
    "shop_distance = pd.read_csv('auxiliary-data/sg-shopping-malls.csv')\n",
    "expected_result = pd.merge(train, secschooldistance, on = 'planning_area')\n",
    "train['shop_distance'] = haversine_np(train['lng'], train['lat'], expected_result['lng_y'], expected_result['lat_y'])\n",
    "\n",
    "# using long and lat\n",
    "shop_distance = pd.read_csv('auxiliary-data/sg-shopping-malls.csv')\n",
    "expected_result = pd.merge(test, secschooldistance, on = 'planning_area')\n",
    "test['shop_distance'] = haversine_np(test['lng'], test['lat'], expected_result['lng_y'], expected_result['lat_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property Type cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hdb 4 rooms', 'hdb', 'condo', 'Condo', 'bungalow', 'Hdb',\n",
       "       'terraced house', 'Hdb Executive', 'apartment',\n",
       "       'Semi-Detached House', 'Apartment', 'Hdb 4 Rooms',\n",
       "       'semi-detached house', 'hdb 3 rooms', 'executive condo',\n",
       "       'corner terrace', 'hdb executive', 'Hdb 3 Rooms', 'Hdb 5 Rooms',\n",
       "       'hdb 5 rooms', 'landed', 'hdb 2 rooms', 'Executive Condo',\n",
       "       'Bungalow', 'Corner Terrace', 'Terraced House', 'cluster house',\n",
       "       'Cluster House', 'Land Only', 'townhouse', 'Hdb 2 Rooms',\n",
       "       'conservation house', 'land only', 'walk-up', 'Townhouse',\n",
       "       'Conservation House', 'good class bungalow', 'Landed', 'shophouse'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.property_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change all to uppercase\n",
    "train['property_type']=train['property_type'].str.upper()\n",
    "\n",
    "#Change all to uppercase\n",
    "test['property_type']=test['property_type'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HDB 4 ROOMS', 'HDB', 'CONDO', 'BUNGALOW', 'TERRACED HOUSE',\n",
       "       'HDB EXECUTIVE', 'APARTMENT', 'SEMI-DETACHED HOUSE', 'HDB 3 ROOMS',\n",
       "       'EXECUTIVE CONDO', 'CORNER TERRACE', 'HDB 5 ROOMS', 'LANDED',\n",
       "       'HDB 2 ROOMS', 'CLUSTER HOUSE', 'LAND ONLY', 'TOWNHOUSE',\n",
       "       'CONSERVATION HOUSE', 'WALK-UP', 'GOOD CLASS BUNGALOW',\n",
       "       'SHOPHOUSE'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.property_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: MAY NEED TO CHECK EACH VALUE INDIVIDUALLY TO SEE IF THERE ARE ANY WITH ONLY 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train['property_type'])\n",
    "train['property_type']=le.transform(train['property_type'])\n",
    "\n",
    "test['property_type']=le.transform(test['property_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '99-year leasehold', 'freehold', '999-year leasehold',\n",
       "       '110-year leasehold', '946-year leasehold', '103-year leasehold',\n",
       "       '956-year leasehold', '929-year leasehold', '102-year leasehold',\n",
       "       '100-year leasehold', '947-year leasehold'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tenure.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tenure'] = train['tenure'].replace(\n",
    "    [\n",
    "    '99-year leasehold',\n",
    "    '110-year leasehold',\n",
    "    '103-year leasehold',\n",
    "    '102-year leasehold',\n",
    "    '100-year leasehold',\n",
    "    ],\n",
    "    '99-year leasehold'\n",
    ")\n",
    "\n",
    "train['tenure'] = train['tenure'].replace(\n",
    "    [\n",
    "    '946-year leasehold',\n",
    "    '999-year leasehold',\n",
    "    '956-year leasehold',\n",
    "    '929-year leasehold',\n",
    "    '947-year leasehold',\n",
    "    ],\n",
    "    '999-year leasehold'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test['tenure'] = test['tenure'].replace(\n",
    "    [\n",
    "    '99-year leasehold',\n",
    "    '110-year leasehold',\n",
    "    '103-year leasehold',\n",
    "    '102-year leasehold',\n",
    "    '100-year leasehold',\n",
    "    ],\n",
    "    '99-year leasehold'\n",
    ")\n",
    "\n",
    "test['tenure'] = test['tenure'].replace(\n",
    "    [\n",
    "    '946-year leasehold',\n",
    "    '999-year leasehold',\n",
    "    '956-year leasehold',\n",
    "    '929-year leasehold',\n",
    "    '947-year leasehold',\n",
    "    ],\n",
    "    '999-year leasehold'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '99-year leasehold', 'freehold', '999-year leasehold'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tenure.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tenure'] = test['tenure'].fillna('99-year leasehold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove NA \n",
    "train = train.dropna(subset=['tenure'])\n",
    "\n",
    "## Remove NA \n",
    "test['tenure'] = test['tenure'].fillna('99-year leasehold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "tenurepp = preprocessing.LabelEncoder()\n",
    "tenurepp.fit(train['tenure'])\n",
    "train['tenure']=tenurepp.transform(train['tenure'])\n",
    "\n",
    "test['tenure']=tenurepp.transform(test['tenure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unspecified', 'partial', 'unfurnished', 'fully', 'na'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.furnishing.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume not specified and na as unfurnished \n",
    "train['furnishing'] = train['furnishing'].replace(\n",
    "    [\n",
    "    'na',\n",
    "    'unspecified',\n",
    "    ],\n",
    "    'unfurnished'\n",
    ")\n",
    "\n",
    "#Assume not specified and na as unfurnished \n",
    "test['furnishing'] = test['furnishing'].replace(\n",
    "    [\n",
    "    'na',\n",
    "    'unspecified',\n",
    "    ],\n",
    "    'unfurnished'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unfurnished', 'partial', 'fully'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['furnishing'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "furnishing = preprocessing.LabelEncoder()\n",
    "furnishing.fit(train['furnishing'])\n",
    "train['furnishing']=furnishing.transform(train['furnishing'])\n",
    "\n",
    "test['furnishing']=furnishing.transform(test['furnishing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## floor_level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too few data for floor level, we should just take out the column\n",
    "train = train.drop(columns='floor_level')\n",
    "\n",
    "# Too few data for floor level, we should just take out the column\n",
    "test = test.drop(columns='floor_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total_num_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "meantotalunit = train['total_num_units'].mean()\n",
    "train['total_num_units'].fillna(meantotalunit, inplace=True)\n",
    "\n",
    "meantotalunit = test['total_num_units'].mean()\n",
    "test['total_num_units'].fillna(meantotalunit, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgbuildyear = round(train['built_year'].mean(),0)\n",
    "train['built_year'] = train['built_year'].fillna(avgbuildyear)\n",
    "\n",
    "avgbuildyear = round(test['built_year'].mean(),0)\n",
    "test['built_year'] = test['built_year'].fillna(avgbuildyear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## num_baths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgbath = round(train['num_baths'].mean(),1)\n",
    "train['num_baths'] = train['num_baths'].fillna(avgbath)\n",
    "\n",
    "avgbath = round(test['num_baths'].mean(),1)\n",
    "test['num_baths'] = test['num_baths'].fillna(avgbath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## num_bedds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgbed = round(train['num_beds'].mean(),1)\n",
    "train['num_beds'] = train['num_beds'].fillna(avgbath)\n",
    "\n",
    "avgbed = round(test['num_beds'].mean(),1)\n",
    "test['num_beds'] = test['num_beds'].fillna(avgbath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subzone&planning zone are very important to price of realestate. So we can drop na listings without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## num_avaliable_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "numavaunittypes = train.available_unit_types.str.count(',') +1\n",
    "train['available_unit_types'] = numavaunittypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ava_units = train['available_unit_types'].mean()\n",
    "train['available_unit_types'] = train['available_unit_types'].fillna(avg_ava_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numavaunittypes = test.available_unit_types.str.count(',') +1\n",
    "test['available_unit_types'] = numavaunittypes\n",
    "\n",
    "avg_ava_units = test['available_unit_types'].mean()\n",
    "test['available_unit_types'] = test['available_unit_types'].fillna(avg_ava_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subzone & planning zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "property_type                7000\n",
       "tenure                       7000\n",
       "built_year                   7000\n",
       "num_beds                     7000\n",
       "num_baths                    7000\n",
       "size_sqft                    7000\n",
       "furnishing                   7000\n",
       "available_unit_types         7000\n",
       "total_num_units              7000\n",
       "lat                          7000\n",
       "lng                          7000\n",
       "subzone                      6967\n",
       "planning_area                6967\n",
       "cc_distance                  7000\n",
       "mrt_distance                 7000\n",
       "primary_school_distance      7000\n",
       "secondary_school_distance    7000\n",
       "shop_distance                7000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['serangoon', 'marine parade', 'bukit timah', 'downtown core',\n",
       "       'bukit batok', 'kallang', 'tanglin', 'bedok', 'bishan', 'novena',\n",
       "       'ang mo kio', 'queenstown', 'pasir ris', 'museum', 'newton', nan,\n",
       "       'yishun', 'orchard', 'bukit panjang', 'sembawang', 'geylang',\n",
       "       'bukit merah', 'sengkang', 'toa payoh', 'clementi', 'outram',\n",
       "       'woodlands', 'hougang', 'singapore river', 'punggol',\n",
       "       'river valley', 'jurong west', 'southern islands', 'tampines',\n",
       "       'rochor', 'jurong east', 'choa chu kang',\n",
       "       'central water catchment', 'tengah', 'lim chu kang', 'changi',\n",
       "       'seletar', 'paya lebar', 'mandai'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['planning_area'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1255\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa = train['planning_area'] + test['planning_area']\n",
    "pa = pa.dropna().drop_duplicates().unique()\n",
    "pa = pd.DataFrame(pa)\n",
    "pa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train['planning_area'].drop_duplicates()\n",
    "b = test['planning_area'].drop_duplicates()\n",
    "c = pd.concat([a, b]).dropna()\n",
    "c = c.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "planningzone = preprocessing.LabelEncoder()\n",
    "planningzone.fit(c)\n",
    "\n",
    "train = train.dropna()\n",
    "train['planning_area']=planningzone.transform(train['planning_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostfreq = test['planning_area'].mode()\n",
    "mostfreq\n",
    "test['planning_area'] = test['planning_area'].fillna('bukit timah')\n",
    "test['planning_area']=planningzone.transform(test['planning_area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAN CONSIDER NOT DROPPING SUBZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns='subzone')\n",
    "test = test.drop(columns='subzone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data MINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "'''Data Prep'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = train.copy()\n",
    "dataY = dataX.pop('price')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important sample code\n",
    "# parameter tuning for random forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params = {'max_features':('sqrt', 'log2'), 'n_estimators':range(10, 100, 5)} # from 10 to 100 with 5 increment\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_jobs=-1, # for using multiple cpu/core to speed up calculation\n",
    "    random_state=2022, # random seed\n",
    "    verbose=0, # print out details or not    \n",
    ")\n",
    "\n",
    "clf2 = GridSearchCV(clf, param_grid=params, scoring='accuracy', cv=5)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# see all output variables in cv results\n",
    "sorted(clf2.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model\n",
    "clf2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(clf2.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(clf2.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
